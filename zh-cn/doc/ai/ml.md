<!--
  Copyright (c) 2018, Xin YUAN, courses of Zhejiang University
  All rights reserved.

  This program is free software; you can redistribute it and/or
  modify it under the terms of the 2-Clause BSD License.

  Author contact information:
    yxxinyuan@zju.edu.cn
-->

# 机器学习

## 前言

机器学习是一门多领域交叉学科，专门研究计算机怎样模拟或实现人类的学习行为，
以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身的性能，
是人工智能的核心，是使计算机具有智能的根本途径，
其应用遍及人工智能的各个领域[参见[机器学习][BDBK-ML]]。
从广义上来说，它是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法，
但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。

## 问题

本章只讨论有监督的机器学习问题，其要解决的问题如下：

1. 回归问题

	回归问题的目标是给定一组n维输入变量x，并且每一个输入的x都有对应的值y，
	我们可以用一条曲线去尽量拟合这些数据点，要求对于这个数据集之外的新输入x'，
	能够预测它对应的连续的目标值t，即返回拟合的曲线上对应的点的值y'从而达到预测的目的。
	要寻找数据和对应连续值之间的关系，实际就是要找到一个函数，能够将数据映射到连续值上。

	回归问题一般通过以下三步解决：
	选择一个模型，模型实际就是函数的集合；
	需要有一个评判标准，能够判断函数的好坏，即loss function(损失函数)，一般它的值越大，
	该函数表现得越差；
	利用上一步中评判标准，在函数集合中找到最好的函数[参见[回归问题][CSDN-RP]]。
	这是一个经典的泛函最优化问题的求解过程。

	以回归问题中最基础的线性回归为例，需要求解的函数可表示为`y=w^Tx+b`，
	其中x和y都是向量，而w和b即为需要求解的两个参数。
	当损失函数为均方误差时，用最小二乘法可以直接计算出w和b的解析解，
	最小二乘法找到的是误差平方和最小的函数：
	![](1.png "损失函数")，其中yi表示实际值，N则是样本个数。
	另一种常用的方法是梯度下降法，这是一种迭代式的算法，
	它的基本思想，就是对损失函数求参数的梯度，向梯度下降的方向更新参数：
	![](2.png "梯度下降")，
	其中L是损失函数，θ是参数构成的向量，η是learning rate（学习率），
	体现的是每次调整参数调整的大小。

1. 分类问题

	分类问题的本质和回归问题一样，区别在于，输出不再是连续值，而是离散值，
	通过寻找一个函数判断输入数据所属的类别，为其打上一个“标签”，用来指定其属于哪个类别。
	分类问题在现实中的应用也非常广泛，有垃圾邮件识别，手写数字识别等。

	分类问题的求解过程同样可以分为三个步骤：
	确定一个模型`f(x)`，输入样本数据x，输出其类别；
	定义损失函数`L(f)`，一个最简单的想法是计数分类错误的次数；
	找出使损失函数最小的那个最优函数[参见[分类问题][CSDN-CP]]。

	逻辑回归（Logistic Regression）可以看做一种特殊的分类算法，分为正向类（1）和负向类（0），
	因此需要一个输出值在 0 和 1 之间的假设函数：![](3.png "逻辑回归")，
	根据设定的参数，输入给定的变量，可以通过此式计算输出变量的值为 1/0 的可能性。
	另一种常见的分类算法是朴素贝叶斯分类，
	通过对给出的待分类项求解各项类别的出现概率大小，来判断此待分类项属于哪个类别，
	通常会选择概率最大的类别，	其实质就是计算条件概率的公式。

1. 决策问题

	决策是一种以示例为基础的归纳学习方法，
	通过每次在各个属性中找到区分度最大的属性来作为当前决策树的节点，
	从而构造出一颗最小体积决策树。
	决策问题中比较重要的就是熵的概念，通常表示事物的混乱程度，熵越大表示混乱程度越大：
	![](4.png "熵")，其中p(xi)为xi为某一取值情况时的概率。

	决策树划分的好坏通过信息增益来计算，
	信息增益就是前后两个熵的差，当差值越大说明按照此划分对于事件的混乱程度减少越有帮助，
	计算各个属性的信息增益，并选择信息增益最大的属性作为当前划分节点。

在数学的形式上，这几个问题是高度相似的，在一定的条件下，其数学模型可以统一表示。
其中，要最优的目标函数，也就是泛函，是需要读者自行分辨其凸性，
从而判断能否得到全局最优解。

## 统计观点

通常首先假设样本空间内存在一批“独立同分布”的样本，服从某个未知分布，
每一条样本都由一组反映事件或对象在某方面的表现或性质的事项组成，也可称作属性-属性值。
样本空间内的每一个点都有其对应的向量表示，
机器学习的任务就是通过这些数据产生“模型”的算法，即“学习算法”（learning algorithm），
当面对“新样本”时，这个“模型”就能根据学习的结果对其进行良好的判断。

可见，机器学习的假设条件是比较严格的，在现实的应用场景下，
获得的训练数据往往并不服从独立同分布假设，同时，模型训练完成后，
输入的新数据若是不符合模型所描述的分布，预测将失败。
因而，在现实非关键应用的工程中，往往把各种可能出现的情形都当作样本加入训练集，
构造尽可能完善的总体样本库来接近独立同分布假设。

## 几何描述

1. 分类问题

<<<<<<<
只以分类问题为例 查出SVM 的几何直观解释（模型空间超平面，最优化问题的函数空间
是类似抛物线的凸函数）
核函数方法，给出一段，并图片几何解释
>>>>>>>

1. 深度学习

<<<<<<<
深度学习用于图像分类问题的基本框架图，有CNN 各个层 激活函数 全链接 sopftmax等。
然后解释这些层就是构造了足够复杂，参数远多于输入维度的模型函数空间，直观就是
高维空间中的曲面，具有流形的性质，是多个欧氏空间的拼接，所以很容易过拟合
》》》》》》》》


## 科学哲学观点

本质上，当前有监督的机器学习还是在拟合具有单一假想有因果关系的函数。
而最优化目标函数的设计思想就是设计体现能量最小化的损失函数，
或者假设因变量和自变量具有相关关系时，构造两者的联合概率分布，
通过贝叶斯方法和最大似然法来设计。

## 小结

可见，当机器学习热点方法所基于的理论和技术依然是经典数学的内容，
并没有实质性的进步。

[BDBK-ML]: https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/217599?fr=aladdin "机器学习"
[CSDN-RP]: https://blog.csdn.net/ivonui/article/details/79263387 "回归问题"
[CSDN-CP]: https://blog.csdn.net/hohaizx/article/details/81835381 "分类问题"
